<!DOCTYPE html>
<!-- saved from url=(0019)http://seretod.org/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		
	<title>FutureDial Home</title>
			<link rel="stylesheet" type="text/css" href="./index_files/web.css">
			
			<link rel="icon" href="./index_files/logo.png" type="image/x-icon">
			<script type="text/javascript">
				window.onload = function(){
					var but = document.getElementById("but");
					but.onclick = function(){
						alert("89746")
					}
				}
			</script>
	<style>
		.green a:link,a:visited{color: #0000EE;text-decoration:underline;}
		.green a:hover{text-decoration: underline;}
		
	</style></head>
	<body>
		<div class="title_bar">
			<div class="title">
				<div class="logo">
					<img src="./SereTOD Accepted Papers_files/logo.png" alt="" style="width: unset !important;;height: 100%;">
				</div>
				<ul style="width:400px;height:60px;float: left;">
					<a style="text-decoration: none;font:400 22px/60px &#39;microsoft yahei&#39;;color:#fff" href="./index.html">FutureDial-RAG 2024 Challenge
				</a></ul><a style="text-decoration: none;font:400 22px/60px &#39;microsoft yahei&#39;;color:#fff" href="./index.html">
				</a><ul class="label" style="float: right;height:60px;"><a style="text-decoration: none;font:400 22px/60px &#39;microsoft yahei&#39;;color:#fff" href="https://future-dial.github.io/SereTOD/">
					</a><a style="text-decoration: none;color:#101010;font: 400 16px/60px &#39;microsoft yahei&#39;;" href="./index.html"><li>Home</li></a>
					<a style="text-decoration: none;color:#101010" href="./Organizers.html"><li>Organizers</li></a>
					<a style="text-decoration: none;color:#101010" href="./Program.html"><li>Program</li></a>
					<!-- <a style="text-decoration: none;color:#101010" href="./SereTOD Challenge.html"><li>Challenge</li></a>
					<a style="text-decoration: none;color:#101010" href="./SereTOD Accepted%20Papers.html"><li>Accepted Papers</li></a>
					
					<a style="text-decoration: none;color:#101010" href="./SereTOD Call%20for%20Papers.html"><li>Call for Papers</li></a> -->
				</ul>
			</div>
		</div>
		<div class="banner">
			<h1 style="font: 700 26px/50px &#39;microsoft yahei&#39;;color:#fff;text-align: center;">The 2nd FutureDial Challenge co-located with SLT 2024
				<br>Dialog Systems with Retrieval Augmented Generation</h1>
			<h1 style="font: 700 26px/80px &#39;microsoft yahei&#39;;color:#fff;text-align: center;margin-top:30px; color:#7dd1fb;">Invited Speakers</h1>
		</div>

		<div id="Dong-Yan Huang" style="width: 63%;height:100%;border:1px solid #dddddd;margin:0 auto;margin-top: 50px;">
			<div style="background-color: #f5f5f5;border-bottom:1px solid #dddddd;">
				<p style="margin-left: 30px;font-size: 16px;color: #333333;padding: 20px 0px;">Dialog Systems for Service Robots</p>
			</div>
			<div style="padding:20px 30px;font-size: 24px;color: #333333"><b>Dong-Yan Huang</b></div>
			<p style="padding: 20px 30px 10px;font-size: 16px;padding-top: 0;text-align: justify;"><b>Abstract:</b> 
                In this talk, starting from the research of natural language processing technology, we will comprehensively analyze the application of natural language processing in robots, such as multi-round dialogue problems in human-robot interaction,  AI writing creation, and a comprehensive analysis of natural language processing on robots, analysis of application cases of language processing on robots.  Then, we will present a comparison study of retrieval augmented generation (RAG),  supervised fine-turing, (SFT), and offline reinforcement learning to mitigate hallucination for multimodal large language models (LLMs). Final, we point out the future research directions of natural language processing technology in human-robot interaction.
			</p>
			<p style="padding: 20px 30px;color: #333333;font-size: 16px;padding-top: 0;text-align: justify;"><b>Bio:</b> 
                Dong-Yan Huang (M’96-SM’05) received her bachelor and master degrees from Xi’an Jiaotong University, Xi’an, China, in 1985 and 1988, respectively, and the PhD degree in Syst`eme Physique et M´etrologie-Communication & Electronique from the Conservatoire National des Arts et M´etiers Paris (CNAM), France, in 1996. She is now a Principal Scientist at UBTECH Robotics Corp. From Dec. 1997 to Nov. 2002, she was a Senior Research Engineer at the Institute of Microelectronics, Singapore. Before that, she was a postdoctoral researcher at the UFR de Math´ematiqueset Informatique, Universit´e Paris Descartes, France. From Dec. 2002 to 2019, she was a Senior Scientist at the Institute for Infocomm Research, Singapore. Her research focuses on machine learning, pattern recognition, affective computing, automatic speech recognition, text-to-speech synthesis, voice conversion, computer vision, dialogue system, talking head, human-machine interaction, robotics and embodiment intelligence. She has authored more than 100 publications in peer-reviewed journals, and conference proceedings. She was solicited and co-chaired for ASMMC from 2015 to 2021. She has been serving as the program committee for several international conferences in the areas of signal processing, speech processing, multimedia, human-computer interaction, affective computing and intelligent interaction. She was the chair of the IEEE Singapore Sensor Committee Sub-Committee (2016-2018), the chair of the WIE (Women in Engineering) group (2006-2008). She led a team to implement online and offline speech technology on Cruzr, Walker robots and a series of educational products. Her team’s works on digital emotion won the awards of A*STAR’s 30 Most Impactful Innovations & Inventions over Three Decades (2021), and P&G Connect + Develop Open Innovation Solutions Award on Digital Insights 2020, the first prize in the 2011 INTERSPEECH Speaker State Challenge Sleep Competition and the first prize in the EmotioNet Challenge.
			</p>
		</div>

		<div id="Hung-yi Lee" style="width: 63%;height:100%;border:1px solid #dddddd;margin:0 auto;margin-top: 50px;">
			<div style="background-color: #f5f5f5;border-bottom:1px solid #dddddd;">
				<p style="margin-left: 30px;font-size: 16px;color: #333333;padding: 20px 0px;">Teaching Foundation Models New Skills: Insights and Experiences</p>
			</div>
			<div style="padding:20px 30px;font-size: 24px;color: #333333"><b>Hung-yi Lee</b></div>
			<p style="padding: 20px 30px 10px;font-size: 16px;padding-top: 0;text-align: justify;"><b>Abstract:</b> 
                In today's landscape of natural language processing (NLP) and speech processing, developing applications often begins with fine-tuning a foundation model. However, teaching a foundation model like LLaMA new skills is not as straightforward as it seems. Introducing new capabilities can often impair their original functions, a phenomenon known as catastrophic forgetting. While experience replay is a common solution, the need for training data for the foundation models poses challenges for continuous training. This talk will delve into recent research on fine-tuning language models, including their spoken counterparts, focusing on preserving their initial capabilities.
			</p>
			<p style="padding: 20px 30px;color: #333333;font-size: 16px;padding-top: 0;text-align: justify;"><b>Bio:</b> 
                Hung-yi Lee is a professor of the Department of Electrical Engineering at National Taiwan University (NTU), with a joint appointment at the Department of Computer Science & Information Engineering of the university. His recent research focuses on developing technology that can reduce the requirement of annotated data for speech processing (including voice conversion and speech recognition) and natural language processing (including abstractive summarization and question answering). He won Salesforce Research Deep Learning Grant in 2019, AWS ML Research Award in 2020, Outstanding Young Engineer Award from The Chinese Institute of Electrical Engineering in 2018, Young Scholar Innovation Award from Foundation for the Advancement of Outstanding Scholarship in 2019, Ta-You Wu Memorial Award from Ministry of Science and Technology of Taiwan in 2019, and The 59th Ten Outstanding Young Person Award in Science and Technology Research & Development of Taiwan.
			</p>
		</div>
		
		<div style="margin-top:140px;" class="footer_bar">
			<div class="footer">
				<p style="color:#fff;font:400 16px/60px &#39;microsoft yahei&#39;;text-align:center;">Copyright © FutureDial-RAG Organizers. All rights reserved.</p>
			</div>
		</div>
    </body>
</html>