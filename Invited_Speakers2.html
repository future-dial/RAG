<!DOCTYPE html>
<!-- saved from url=(0019)http://seretod.org/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		
	<title>FutureDial Home</title>
			<link rel="stylesheet" type="text/css" href="./index_files/web.css">
			
			<link rel="icon" href="./index_files/logo.png" type="image/x-icon">
			<script type="text/javascript">
				window.onload = function(){
					var but = document.getElementById("but");
					but.onclick = function(){
						alert("89746")
					}
				}
			</script>
	<style>
		.green a:link,a:visited{color: #0000EE;text-decoration:underline;}
		.green a:hover{text-decoration: underline;}
		
	</style></head>
	<body>
		<div class="title_bar">
			<div class="title">
				<div class="logo">
					<img src="./SereTOD Accepted Papers_files/logo.png" alt="" style="width: unset !important;;height: 100%;">
				</div>
				<ul style="width:400px;height:60px;float: left;">
					<a style="text-decoration: none;font:400 22px/60px &#39;microsoft yahei&#39;;color:#fff" href="./index.html">FutureDial-RAG 2024 Challenge
				</a></ul><a style="text-decoration: none;font:400 22px/60px &#39;microsoft yahei&#39;;color:#fff" href="./index.html">
				</a><ul class="label" style="float: right;height:60px;"><a style="text-decoration: none;font:400 22px/60px &#39;microsoft yahei&#39;;color:#fff" href="http://seretod.org/">
					</a><a style="text-decoration: none;color:#101010;font: 400 16px/60px &#39;microsoft yahei&#39;;" href="./index.html"><li>Home</li></a>
					<a style="text-decoration: none;color:#101010" href="./Organizers.html"><li>Organizers</li></a>
					<a style="text-decoration: none;color:#101010" href="./Program.html"><li>Program</li></a>
					<!-- <a style="text-decoration: none;color:#101010" href="./SereTOD Challenge.html"><li>Challenge</li></a>
					<a style="text-decoration: none;color:#101010" href="./SereTOD Accepted%20Papers.html"><li>Accepted Papers</li></a>
					
					<a style="text-decoration: none;color:#101010" href="./SereTOD Call%20for%20Papers.html"><li>Call for Papers</li></a> -->
				</ul>
			</div>
		</div>
		<div class="banner">
			<h1 style="font: 700 26px/50px &#39;microsoft yahei&#39;;color:#fff;text-align: center;">The 2nd FutureDial Challenge co-located with SLT 2024
				<br>Dialog Systems with Retrieval Augmented Generation</h1>
			<h1 style="font: 700 26px/80px &#39;microsoft yahei&#39;;color:#fff;text-align: center;margin-top:30px; color:#7dd1fb;">Invited Speakers</h1>
		</div>
		<div id="Hung-yi Lee" style="width: 63%;height:100%;border:1px solid #dddddd;margin:0 auto;margin-top: 50px;">
			<div style="background-color: #f5f5f5;border-bottom:1px solid #dddddd;">
				<p style="margin-left: 30px;font-size: 16px;color: #333333;padding: 20px 0px;">Teaching Foundation Models New Skills: Insights and Experiences</p>
			</div>
			<div style="padding:20px 30px;font-size: 24px;color: #333333"><b>Hung-yi Lee</b></div>
			<p style="padding: 20px 30px 10px;font-size: 16px;padding-top: 0;text-align: justify;"><b>Abstract:</b> 
                In today's landscape of natural language processing (NLP) and speech processing, developing applications often begins with fine-tuning a foundation model. However, teaching a foundation model like LLaMA new skills is not as straightforward as it seems. Introducing new capabilities can often impair their original functions, a phenomenon known as catastrophic forgetting. While experience replay is a common solution, the need for training data for the foundation models poses challenges for continuous training. This talk will delve into recent research on fine-tuning language models, including their spoken counterparts, focusing on preserving their initial capabilities.
			</p>
			<p style="padding: 20px 30px;color: #333333;font-size: 16px;padding-top: 0;text-align: justify;"><b>Bio:</b> 
                Hung-yi Lee is a professor of the Department of Electrical Engineering at National Taiwan University (NTU), with a joint appointment at the Department of Computer Science & Information Engineering of the university. His recent research focuses on developing technology that can reduce the requirement of annotated data for speech processing (including voice conversion and speech recognition) and natural language processing (including abstractive summarization and question answering). He won Salesforce Research Deep Learning Grant in 2019, AWS ML Research Award in 2020, Outstanding Young Engineer Award from The Chinese Institute of Electrical Engineering in 2018, Young Scholar Innovation Award from Foundation for the Advancement of Outstanding Scholarship in 2019, Ta-You Wu Memorial Award from Ministry of Science and Technology of Taiwan in 2019, and The 59th Ten Outstanding Young Person Award in Science and Technology Research & Development of Taiwan.
			</p>
		</div>
		<div style="margin-top:140px;" class="footer_bar">
			<div class="footer">
				<p style="color:#fff;font:400 16px/60px &#39;microsoft yahei&#39;;text-align:center;">Copyright Â© FutureDial-RAG Organizers. All rights reserved.</p>
			</div>
		</div>
    </body>
</html>